Title:	Kana-kanji determination
Stat:	open
Disp:	
Prio:	med
Cats:	dev
Reqtrs:	
AssdTo:	stuart@friix.com

2007-02-01 00:00:00 stuart@friix.com
  jmdict.pm -- The database expects text strings in table "rdng" to
  contain no kanji chars and in table "kanj" to contain at least one.
  Function jstr_classify used to determine kana/kanji-ness.  However there
  are some jmdict items that have kanji text that are not considered kanji
  by jmdict::jstr_classify:
  1000060: 々
  1000090: ◎
  1000090: ×
  1085560: トライ＆エラー (is reading but search thinks it's kanji)
  Change jstr_classify?  Or do something else?

  Also, jelparse.y uses kanji/kana test to identify kanji and reading
  sections, and also xrefs.  In 1000090, the xref [ant=×] gets mis-
  interpreted as a tag (like "pos=vs") because the "×" character is
  neither kanji or kana and results in a parse failure with error,
  "ANT is unrecognised tag type".

2007-02-01 00:00:00 stuart@friix.com
  See also IS-83: "brackets in reading cause parse error".

2008-04-25 17:01:00 stuart@friix.com
  Added check to jmparse.py that issues a warning to the log file if
  a <keb> element is processed that does not contain any kanji chars.
  When run on JMdict or JMnedict, a lot of such warnings occur, most
  due to failure of jdb.jstr_classify() to treat full-width equivalents
  of ascii characters as kanji.  In jdb.jstr_classify() changed the 
  unicode range defining "kanji" from 0x4E00-0x9FFF to 0x4E00-0xFFFF.
  However, the extended range is probably not right either as is 
  contains half-width katakana (FF61-FF9F) and the unicode surrogate
  range (D800-DFFF).

  There are similar warnings from JMnedict but in addition there are 
  a lot of kana-only <keb> texts.  See the following explanation by
  Jim Breen on the Edict mailing list:
    Subject: Re: [edict-jmdict] jmnedict non-kanji keb elements 
    Date: 2008-04-24
    http://tech.groups.yahoo.com/group/edict-jmdict/message/2547
  Will either wait for Jim to fix in JMnedict, or will have to try
  to detect and relocate during parsing.

  Perhaps we need some code from jstr_classify() that indicates
  characters that could be *either* kanji or kana?

2008-04-25 23:59:27 stuart@friix.com
  Just noticed that the above change fixes many "no kanji in keb"
  warnings when processing jmdict, it now causes many "no kana in 
  in "reb" warning when processing jmnedict!
  If seems that the full-width ascii strings put in keb elements in
  jmdict are in reb lements in jmnedict. 

  Need to think about how to fix this.

2008-06-23 21:49:00 stuart
  In Edict list thread:
    2008-04-23: [edict-jmdict] jmnedict non-kanji keb elements
  JB said he'd changed mixed kana/FWascii ken element to reb elements
  in JMnedict.

  On the same issue in JMdict in:
    2008-04-26: [edict-jmdict] jmdict/jmnedict inconsistency
  his response was:
    "OK, I am now accepting full-width alphabetics as keb members."

  Wrote a quicky script to scan a jmdict file, and the find unicode
  character blocks(s) for the character sin the reb, keb, and gloss
  elements.  Results in moinmoin docs: 
    http://localhost:8080/KanjiReadingDiscrimination

2008-06-30 17:19:00 stuart
  In Edict list thread:
    2008-06-27: [edict-jmdict] jmnedict non-kanji keb elements
  Jim Breen wrote:

  > If it contains ONLY characters from the set
  > <hiragana, katakana, ヽ, ヾ, ゝ, ゞ, ー> AND
  > it has at least one kana, it goes into the <reb>. All
  > others go into the <keb>
  > 
  > > (An aside... what is the rule for numbers in readings,
  > > should they be given as western-style digits (２１),
  > > spelled out in kana (にじゅういち), both?, either? Does
  > > it depend on the counter, 21っぷん、にじゅういっぷん？)
  > 
  > Strictly the ２１ should be in the <keb>, perhaps with
  > 二十一 as well, and the matching にじゅういち in the <reb>.
  > 
  > Anything that is *purely* a reading or a kana-only formation
  > goes in the <reb>. All others, go in the <keb>. Does that make
  > sense?

  Modified jdb.jstr_classify() to incorporate the definition JB
  suggested, and added three new functions, jstr_reb(), jstr_keb(),
  and jstr_gloss() since those determinations are now more complex
  than simple bit masking.

  Also modified lib/jmxml.py, lib/jellex.py, autocond() in lib/jdb.py,
  and progam exparse.py to use the new functions rather than 
  jstr_classify.

2008-07-01 11:14:00 stuart
  Added tests for revised jdb.jstr_classify() function (IS-8).

2009-02-26 08:34:00 stuart
  From email to the jmdict list from Jim Breen, 2009-02-26:
  >[...]
  > I propose to state some rules for what can go
  > in the "reading" fields. They are:
  > 
  > - kana
  > - the kana-related specials: ー ヽ ヾ ゝ ゞ
  > - ・
  > - ～
  > 
  > All the others, such as double-width alphanumerics are invalid
  > in readings, but can appear in the "kanji" part. Also, anything
  > that fits that set above has to go in the reading part.
  > 
  > Thus for ＩＣカード, etc. it must be "ＩＣカード [アイシーカード]"
  >[...]

2009-02-27 21:33:00 stuart
  All the characers above except ～ (FF5E) are in the unicode code
  blocks, 3040..309F:Hiragana, 30A0..30FF:Katakana.  Emailed response
  to Jim's mail asking is making the rules to be those two blocks
  would be better.

  Updated jdb.jstr_classify() and jdb_jstr_reb() to use the rule 
  3040-30FF+FF5E and parsed latest jmdict, number of misclassifications 
  was wat down, but still a few rebs contained '＆', '、', '／', and 
  full-width alphanumerics.  Email the results to the list.

2009-02-28 13:24:00 stuart
  Jim Breen on the Edict mailing list says that the '〜' character
  in his email is not U+FF5E (FULLWIDTH TILDE) but rather what is
  currently in JMdict XML, U+301C (WAVE DASH), and gives, 
    http://en.wikipedia.org/wiki/Unicode#Mapping_to_legacy_character_sets 
  as the reason why.  Actually, the reference given there, 
    http://std.dkuug.dk/jtc1/sc2/wg2/docs/n2166.doc
  was a little more informative for me, and my understanding is that
  the WAVE DASH character is in JIS X 208 and the FULLWIDTH TILDE is
  not (although it is in JIS X 213.)

  Updated jdb.jstr_classify() for this change.

2009-03-02 07:41:00 stuart
  Added reb/keb check to python/exparse.py and put and jp sentences that
  meet the new reb criteria in the reb (with a warning message) rather
  than blindly in the keb.  After a reload however, none met the criteria.

2009-03-09 10:15:00 stuart
  python/lib/jdb.py: Make functions jstr_reb(), jstr_keb(), jstr_gloss()
  return True is the parameter string has a liength of 0.
  Update the failing tests in python/tests/test_jdb_jstr_classify.py for
  the "reb" rules change described above.  All jstr_clasify tests pass now.

2009-03-13 15:10:00 stuart
  Closing this issue.
  <reb> elements must contain strings consisting of characters in the 
  set U+3040-U+30FF,U+301C.  <keb> can contain any string except those
  that meet the <reb> condition.  
  The jmdictdb database does not enforce these conditions but 
  jdb.jstr_classify(), .jstr_reb(), .jstr_keb(), and .jstr_gloss() can
  test these conditions and allow applications to warn or prevent
  users from storing non-conformant strings.

  The Postgresql regex expression "r.txt !~ E'^[ぁ-ヾ～]+$'" will identify
  a reb-compatible rdng text string.

2010-06-06 14:04:00 stuart
  In python/lib/edparse.py, parse_ritem(), disabled a test
  for jdb.jstr_reb() because jwb calls edparse (via cgi/edform.py's
  'j' option) with a reading of a "?" (or maybe that is a JIS 
  question mark.)

2010-08-13 22:50:00 stuart
  Reopened this IS because in 2010-08-11 private email, Jean-Luc
  Leger reported that he was unable to create a reading restr
  to kanji "・" because the jelparse rejects it as non-kanji.
  "・" is the kanji in entry 2529240 (and also 2529250 and 
  1424310.)

  In Note 2008-06-30 17:19:00, I note jwb proposal of the set
  hitagana + katakana + as reb, with any strings not including
  any other characters as going in keb.  I subsequently proposed 
  on the edict list extending that to the full unicode HIRAGANA
  AND KANAKANA blocks that would also include KATAKANA-MIDDOT, 
  which was accepted by jwb at the time and acknowlaged implicitly 
  in the reference in Note 2009-02-26 08:34:00 which included the 
  middot in the reb character set. 

  Posted a note to the edict list asking for clarification on the 
  status of middot character.

2010-08-14 14:48:00 stuart
  * Changed jdb.py:jstr_classify() to classify middot character as 
    a KSYM rather than KANA character.
  * Change jstr_reb() to return true if string contains at least one
    kana character and nothing except kana or ksym characters.
    (formerly was kana characters only but that included middot.)
  * jstr_gloss() to remains a latin-only string.
    jstr_keb() remains any string that does not fit jstr_reb() or
    jstr_gloss() although the logic is done in jstr_keb() rather
    that calling those two functions.
  test_jdb_jstr_classify.py adjusted to match the new behavior.

2012-12-12 19:02:00 stuart
 The following info was collected as part of investigating IS-222:

 Use of CJK Symbols and Punctuation characters (range U+3000 - U+303F)
 excluding 々 (U+3005) where it occurs with other kanji (100's of entries)
 in jmdict and jmnedict kanj.txt.
 Sql used for search:
    select e.seq,e.src,txt from kanj k join entr e on e.id=k.entr 
      where src!=3 and txt similar to '%[　-〄〆-〿]%' or txt='々'
      order by id;
 (Note that the space in the brackets is a U+3000 space character.)

   seq   | src |           txt            
 --------+-----+--------------------------
 1000040 |   1 | 〃
 1000090 |   1 | 〇
 1000060 |   1 | 々
 1436570 |   1 | 〆る
 1594590 |   1 | 〆切
 1594590 |   1 | 〆切り
 1594600 |   1 | 〆切る
 1853450 |   1 | 〆
 1881510 |   1 | 〆切日
 2055680 |   1 | 〆粕
 2083160 |   1 | 〒
 2086620 |   1 | 見ヶ〆料
 2086620 |   1 | 見ケ〆料
 2146000 |   1 | 〇〇
 2170380 |   1 | ストロンチウム九〇
 2224860 |   1 | 申し込み〆切
 2415870 |   1 | 〜
 2570040 |   1 | 朝焼けは雨、夕焼けは晴れ
 2617550 |   1 | コバルト六〇
 2651930 |   1 | 活〆
 2651930 |   1 | 活け〆
 2741050 |   1 | 〇×
       4 |   2 | 〆丸
       5 |   2 | 〆子
       6 |   2 | 〆治
       7 |   2 | 〆代
       8 |   2 | 〆谷
       9 |   2 | 〆木
      10 |   2 | 〆野
      11 |   2 | 〆祐
   78717 |   2 | モーニング娘。
   78790 |   2 | モー娘。
  146253 |   2 | 下〆
  163678 |   2 | 荷〆峠
  200053 |   2 | 菊乃家〆丸
  220785 |   2 | 金〆
  273462 |   2 | 甲〆
  279456 |   2 | 高〆
  287495 |   2 | 黒、岩池
  290496 |   2 | 根〆
  312589 |   2 | 傘〆
  382326 |   2 | 上〆
  420979 |   2 | 世〆子
  421647 |   2 | 瀬〆
  422978 |   2 | 勢〆
  506370 |   2 | 中〆
  580050 |   2 | 馬〆
  580051 |   2 | 馬〆
  580052 |   2 | 馬〆
  580053 |   2 | 馬〆
  618380 |   2 | 文〆
 (51 rows)

 Query for rdng including 々:
 Sql:
    select e.seq,e.src,txt from rdng r join entr e on e.id=r.entr
    where src!=3 and txt similar to '%[　-〿]%' order by id;
 (Note that the space in the brackets is a U+3000 space character.)

   seq   | src |                  txt                   
 --------+-----+----------------------------------------
 2147960 |   1 | モワァ〜ン
 2570040 |   1 | あさやけはあめ、ゆうやけははれ
 2730550 |   1 | あぼ〜ん
       2 |   2 | 〆
       3 |   2 | 〆ヱ
   13580 |   2 | ウィリアム「バッファロービル」コーディ
   15804 |   2 | エグゼクティブ　デシジョン
   17339 |   2 | エンパイア　レコード
   17483 |   2 | オースティンパワーズ　ゴールドメンバー
 (9 rows)

 Checking the examples" corpus and excluding "。" and "、":
    select e.seq,e.src,txt from rdng r join entr e on e.id=r.entr
    where src=3 and txt similar to '%[　-〿]%' 
      and txt not like '%。%' and txt not like '%、%' 
    order by id;

     seq      | src |                       txt                        
 -------------+-----+--------------------------------------------------
 328986074733 |   3 | 「おさきにしつれいします」「おつかれさまでした」
  39393202181 |   3 | 「テニスをしましょうか」「ええ」

 Same check on my personal mnn corpus shows these characters in rdng:
  〜「」［］〜V.te（）／…？，

 Ascii characters:
    kanjidict.rdng: has many ascii "."s to denote okurigana.
    kanjidict.kang: <none of course>
    jmdict.kanj: β-カロテン, α-ｈｅｌｉｘ, 插入(oK) (7 total)
    jmdict.rdng: <none> 
    jjmnedict.kanj: <none>
    jmnedict.rdng: 3 total, all look like errors, e.g. こうのとりのゆりかご)
    examples: 10分だけ..., Windows95対応の...,

2012-12-14 11:43:00 stuart
 Gave up trying to reengineer the jstr_classify code for now.  Made
 some minmal code changes to jdb.jstr_classify to address fix the 
 IS-222 problem.  Will revisit the reengineering attempt later.

 I think the conclusion at this point is that even if the jmdict and
 jmnedict corpora adopt policies that allow the definitive classification
 of japanese text as kanji ("keb") or reading ("reb") text, that cannot
 be assured for other corpora.  For example, jmnedict's rules consider
 digits in a japanese text string (e.g. ６３ビール) as identifying keb
 text yet in many other sources of japanese text, that would be perfectly
 acceptable reading text. 

 Ultimately the idea of trying to reliably determine if a string is
 kanji or reading by looking only at the string is not doable.  We 
 should instead remove from the code any need to make such a determination,
 except possibly as a hueristic for the purpose of generating advisory
 warning messages.  For example, for restr and xref resolution, we could
 look for the given text in both kanj and rdng tables.  Sadly, a side
 effect in the JEL parser will be that syntax errors that formerly could 
 be detected very close to their source in the input text will now be
 semantic errors only discovered after an input is fully parsed.

